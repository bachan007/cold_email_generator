{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this file i will use the groq api.\n",
    "# i am giving the personal information of othe person in plain text and then teh model will summarize the document and store into the text format.\n",
    "# once the data is saved then i will use that data in simple plain text and create the cold email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=os.environ['GROQ_API_KEY']\n",
    "\n",
    "llm = ChatGroq(groq_api_key=groq_api_key,\n",
    "               model_name = \"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_to_file(response, filename):\n",
    "    with open(rf'summary/{filename}.txt', 'w') as file:\n",
    "        file.write(response)\n",
    "    print(f\"Response has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_info = \"\"\"\n",
    "Personal Information:\n",
    "\n",
    "Name: Bachan Nigam\n",
    "Title: Azure Data Engineer\n",
    "Phone: +917905562895\n",
    "Email: bachannigam.nigam111@gmail.com\n",
    "LinkedIn: https://www.linkedin.com/in/bachan-nigam-5187577a/\n",
    "\n",
    "Professional Summary:\n",
    "Accomplished Data Engineer at Procter & Gamble with a strong track record in managing extensive E-commerce data. Proficient in ETL operations, developing robust data pipelines, and distributing data across multiple storage platforms.\n",
    "\n",
    "Work Experience:\n",
    "\n",
    "Tranzita Systems, Lucknow—Azure Data Engineer (May 2022 - Present)\n",
    "\n",
    "Project: iDAS\n",
    "\n",
    "intelligent Decision Automation System, evaluates and recommends adjustments to raw material purchase orders based on changes in finished goods demand forecasts.\n",
    "It assists planners in determining whether to accept these recommendations, ensuring efficient decision-making.\n",
    "Roles and Responsibilities:\n",
    "\n",
    "Design and implement Databricks pipelines for data integration from ADLS, Azure SQL DB, SharePoint, and DataHub.\n",
    "Develop rule-based logic for raw material recommendations and gather supply chain requirements through site visits.\n",
    "Technologies Used: Python, Pandas, PySpark, APIs, ADF, Databricks\n",
    "\n",
    "\n",
    "Project: BRS\n",
    "Branch Replenishment System, The process used to manage and optimize the supply of goods or inventory to different branches of\n",
    "P&G distributors at India Level. Manages and optimizes the supply of goods or inventory to different branches of P&G distributors at the India level.\n",
    "\n",
    "Roles and Responsibilities:\n",
    "\n",
    "Developed robust data pipelines using PySpark on Databricks to extract, transform, and load data between Blob Storage, APIs, Delta Tables, SQL Server DB, and ADLS.\n",
    "Utilized Azure Data Factory (ADF) for efficient data transfer and integrity across sources.\n",
    "Written the Core Logic API to Replenish the Stock from Hub to Branches and multiple reports logic APIs using python and deployment on Azure Functions.\n",
    "Technologies Used: Python, Pandas, PySpark, APIs, ADF, Pyspark, CosmosDB\n",
    "\n",
    "Automated Reporting and Analysis:\n",
    "Implemented automated solutions to enhance reporting accuracy and optimize inventory management using Azure services and Python.\n",
    "Generated color-coded reports comparing regular and advance orders using data from Azure Synapse and SQL DB.\n",
    "Automated root cause analysis for out-of-stock scenarios with Azure Functions, Logic Apps, and SMTP.\n",
    "Streamlined product availability verification, reducing manual efforts by 75-80% and processing time by 90%.\n",
    "Automated E-commerce order audits using Pandas and KNIME, achieving 100% record iteration.\n",
    "Technologies: Azure Functions, Azure Synapse, SQL DB, ADLS, ADF, AAD, SQLAlchemy, Logic Apps, SMTP, Python, Pandas, openpyxl, KNIME, PySpark, Databricks\n",
    "\n",
    "Web Scraping and Visualization: Demonstrated prowess in web scraping by proficiently employing requests, scrapy, bs4, pandas,\n",
    "numpy, and pyodbc. Leveraged the capabilities of Plotly Dash, Seaborn and matplotlib to create engaging data analysis and visualization dashboards.\n",
    "\n",
    "\n",
    "Skills:\n",
    "\n",
    "Programming: Python, R, PySpark\n",
    "Cloud: Azure Functions, Azure Databricks, Azure Data Factory, Azure Active Directory, ADLS, Azure SQL, Azure Synapse, Logic Apps, Cosmos DB\n",
    "Soft Skills: Communication, Problem-Solving, Critical Thinking, Leadership, Logical Reasoning\n",
    "\n",
    "\n",
    "Other Projects/ Personal Projects:\n",
    "Cold Email Generation: The Cold Email Generator project aims to automate the creation of professional and personalized cold emails for job applications. This tool leverages natural language processing to extract relevant information from resumes and company websites, generating tailored emails that highlight the candidate's qualifications and align with the company's needs.\n",
    "The Cold Email Generator is built using Python and Streamlit, providing an interactive web interface for users to upload resumes and input company URLs. The application processes these inputs to extract key information and generate a cold email. The project utilizes various modules and techniques from the LangChain community, such as document loaders, text splitters, embeddings, and retrieval chains, to efficiently handle and analyze the provided data.\n",
    "Tools Used: langchain, Ollama, FAISS, Streamlit, Python3\n",
    "\n",
    "Yfinance Exploration: The goal of the yfinance-exploration project is to streamline the process of gathering detailed stock information, which typically requires extensive research and effort. This project automates the retrieval and compilation of stock data into a single PDF file, making it more efficient and user-friendly.\n",
    "By leveraging Python scripts and libraries, it simplifies the process of gathering and compiling stock information, providing a valuable tool for investors and analysts to make informed decisions with ease. This project exemplifies the integration of programming and finance, demonstrating a practical application of data engineering skills.\n",
    "Tools Used: yfinance, beautifulsoup4, openpyxl, pandas \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_candidate_info = \"\"\"\n",
    "\n",
    "\n",
    "    Please provide a detailed summary of the candidate's professional profile from the resume. Focus on the following aspects:\n",
    "\n",
    "    1. Candidate’s Name: The full name of the candidate.\n",
    "    2. Candidate’s Designation: The current designation of the candidate that would be clearly mentioned in the resume.\n",
    "    3. Industry: The industry or field in which the candidate has worked.\n",
    "    4. Professional Summary: A concise overview of the candidate’s expertise, key skills, and areas of specialization.\n",
    "    5. Key Skills: A list of the candidate's main technical and soft skills.\n",
    "    6. Work Experience: Highlights of the candidate's most recent work experience, including notable projects and accomplishments.\n",
    "    7. Relevant Projects: Information about significant personal or professional projects that demonstrate the candidate's capabilities and achievements with the used technologies.\n",
    "    8. Achievements: Notable accomplishments or achievements listed in the document.\n",
    "    9. Career Objectives: The candidate’s professional goals and what they are looking for in their next role.\n",
    "    10. Linkedin Profile: Linkedin Profile Link that will be like https://www.linkedin.com/candidate-unique-identifier/.\n",
    "\n",
    "\n",
    "    Provide the extracted information in the following format:\n",
    "\n",
    "    Candidate’s Name: [Name]\n",
    "    Industry: [Industry]\n",
    "    Key Skills: [Skill 1], [Skill 2], [Skill 3]\n",
    "    Achievements: [Achievement 1], [Achievement 2]\n",
    "    Current Role: [Role] at [Current Company]\n",
    "    Relevant Projects: Information about significant personal or professional projects that demonstrate the candidate's capabilities and achievements.\n",
    "    Linkedin Profile Link: If there is linkedin profile link then return the link else do not return.\n",
    "\n",
    "    Make sure to carefully review the document and provide accurate and complete details as thee document is divided in two sections.\n",
    "    one is professional projects and another is personal projects.\n",
    "    Do not return howm many years of experience a candidate have.\n",
    "        \"\"\"\n",
    "\n",
    "candidate_info_extraction = llm.invoke(f\"{prompt_for_candidate_info}\\n{personal_info}\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_response_to_file(candidate_info_extraction, filename='bachan_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file can also be used to store the summary of companies as well"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
